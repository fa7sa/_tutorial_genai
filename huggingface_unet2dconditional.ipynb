{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b76bef1-d323-4f4b-a402-e0d960150a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fa7sa/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import UNet2DConditionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "823ff3ee-abfa-466d-8aaf-04eee38b128e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 160,505\n",
      "Trainable parameters: 160,505\n"
     ]
    }
   ],
   "source": [
    "model = UNet2DConditionModel(\n",
    "    sample_size=(45,1),             # size of the generated image\n",
    "    in_channels=1,              # input channels (e.g., latents or features)\n",
    "    out_channels=1,             # output channels\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(4,8,16, 16),  # number of channels in each block\n",
    "    down_block_types=(\n",
    "       \"DownBlock2D\",\"DownBlock2D\",\"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "         \"CrossAttnUpBlock2D\",\"CrossAttnUpBlock2D\",\"UpBlock2D\", \"UpBlock2D\",\n",
    "    ),\n",
    "    norm_num_groups=4,\n",
    "    cross_attention_dim=16  # size of the text embedding or conditioning vector\n",
    ")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d7b3d35-d5b9-41aa-bebf-ffded8191bdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 45, 1])\n",
      "torch.Size([32, 1, 45, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32,1,45,1)\n",
    "print(x.shape)\n",
    "\n",
    "timesteps = torch.randn(32)\n",
    "\n",
    "cond_low = torch.randn(32,45,16)\n",
    "\n",
    "out = model(x, timestep=timesteps, encoder_hidden_states=cond_low)\n",
    "print(out.sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b05d225f-a7cb-449b-b429-a0332e215272",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 64, 1])\n",
      "torch.Size([32, 1, 64, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32,1,64,1)\n",
    "print(x.shape)\n",
    "\n",
    "timesteps = torch.randn(32)\n",
    "\n",
    "cond_low = torch.randn(32,1,16)\n",
    "\n",
    "out = model(x, timestep=timesteps, encoder_hidden_states=cond_low)\n",
    "print(out.sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6280f682-1e51-49d2-b275-9d087a90e5e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "height = 45\n",
    "width = 1\n",
    "\n",
    "# Input (e.g., noisy latent image)\n",
    "x = torch.randn(batch_size, 1, height, width)\n",
    "\n",
    "# Timesteps (e.g., from diffusion schedule)\n",
    "timesteps = torch.tensor([10, 20], dtype=torch.long)\n",
    "\n",
    "# Dummy conditioning vector (e.g., text embedding, energy embedding, etc.)\n",
    "encoder_hidden_states = torch.randn(batch_size, 1, 128)  # (batch, seq_len, cross_attention_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c89c6354-e5c4-42f7-9933-432bc42abdcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One with low conditioning value\n",
    "cond_low = torch.full((batch_size, 1, 16), 0.1)\n",
    "\n",
    "# One with high conditioning value\n",
    "cond_high = torch.full((batch_size, 1, 16), 100.0)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "cond_rand1 = torch.randn(batch_size, 1, 16)\n",
    "\n",
    "torch.manual_seed(7)\n",
    "cond_rand2 = torch.randn(batch_size, 1, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9fe2c20a-7084-424e-a951-334e71fe6026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out_low = model(x, timestep=timesteps, encoder_hidden_states=cond_low)\n",
    "    out_high = model(x, timestep=timesteps, encoder_hidden_states=cond_high)\n",
    "    out_rand1 = model(x, timestep=timesteps, encoder_hidden_states=cond_rand1)\n",
    "    out_rand2 = model(x, timestep=timesteps, encoder_hidden_states=cond_rand2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fa98859e-495f-4969-82dd-8221a24ecefa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 45, 1])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_low.sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9ed96ab7-4dda-455e-99c4-4140455dd9a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg difference between low and high condition outputs: 0.05425228178501129\n"
     ]
    }
   ],
   "source": [
    "diff = torch.mean(torch.abs(out_low.sample - out_high.sample))\n",
    "print(\"Avg difference between low and high condition outputs:\", diff.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "315f02a0-6db3-4a8d-95ab-7095a5e5bb5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensor\n",
    "before_x = torch.randn(2, 4, 1)  # (batch=16, layers=45, channel=1)\n",
    "\n",
    "# Reshape it\n",
    "after_x = before_x.permute(0, 2, 1).unsqueeze(-1)  # â†’ (16, 1, 45, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf03831c-ad76-4f7a-84c6-c4e6024c2748",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a719fca3-dd73-4fe9-a408-ff882e21a757",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafac450-2a5e-45c1-b129-302442565b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ardiff",
   "language": "python",
   "name": "ardiff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
